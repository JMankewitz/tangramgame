{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.attrs import POS\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_utils import get_feats, lemmatize_doc\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import sparse2full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-process text by lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_raw = pd.read_csv('../deidentified/combined.csv', encoding='latin-1')#.rename(index=str, columns={\"contents\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_raw['text'] = [[token for token in nlp(text) if not token.is_stop] for text in d_raw['utterance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_raw['lemmas'] = [lemmatize_doc(parsed_text) for parsed_text in d_raw['text']]\n",
    "docs_dict = Dictionary(d_raw['lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d_raw.copy()\n",
    "gameidList = pd.unique(d.subid.ravel()).tolist()\n",
    "tangramList = pd.unique(d.target.ravel()).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at where conventions were introduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first-order case is where you just look at how often final words occurred at each previous round (percentage).\n",
    "\n",
    "Then we can look at *first* round for each particular word... \n",
    "\n",
    "Then we can check who was speaker on that first round.\n",
    "\n",
    "TODO: check cases where the 'matcher' on a round may have introduced a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "# For each game, look at referring expressions produced by director on final round\n",
    "for name, final_df in d.query('rep_num == 4 and role == \"director\"').groupby('subid') :\n",
    "    final_df = final_df.sort_values('target').reset_index()\n",
    "    round1_df = d.query('rep_num == 1 and role == \"director\" and subid == \"{}\"'.format(name)).sort_values('target').reset_index()\n",
    "    round2_df = d.query('rep_num == 2 and role == \"director\" and subid == \"{}\"'.format(name)).sort_values('target').reset_index()\n",
    "    round3_df = d.query('rep_num == 3 and role == \"director\" and subid == \"{}\"'.format(name)).sort_values('target').reset_index()\n",
    "    \n",
    "    # For each word used with each tangram, check whether it occured in each earlier round\n",
    "    for i, row in final_df.iterrows() :\n",
    "        target = row['target']\n",
    "        words = np.unique(row['lemmas'])\n",
    "        for j, word in enumerate(words) :\n",
    "            round_1_match = word in np.array(list(round1_df.query('target == \"{}\"'.format(target))['lemmas'])).flatten()\n",
    "            round_2_match = word in np.array(list(round2_df.query('target == \"{}\"'.format(target))['lemmas'])).flatten()\n",
    "            round_3_match = word in np.array(list(round3_df.query('target == \"{}\"'.format(target))['lemmas'])).flatten()\n",
    "            rows.append([row['subid'], row['target'], row['person'], row['age'], word, round_1_match, round_2_match, round_3_match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(rows,\n",
    "    columns = ['subid', 'target', 'final_round_person', 'age', 'word', '1_match', '2_match', '3_match']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.to_csv('../deidentified/word_matches.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create tf-idf weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_corpus = [docs_dict.doc2bow(doc) for doc in d['lemmas'] if not np.any(pd.isna(doc))]\n",
    "model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)\n",
    "docs_tfidf  = model_tfidf[docs_corpus]\n",
    "docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])\n",
    "tfidf_emb_vecs = np.vstack([nlp(docs_dict[i]).vector for i in range(len(docs_dict))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_emb_raw = np.dot(docs_vecs, tfidf_emb_vecs) \n",
    "docs_emb = np.insert(docs_emb_raw, nan_insert_rows, np.nan, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine semantic embeddings\n",
    "We'd like to pull out bag of words embeddings from NPs in each utterance in the cued dataset and cluster them for each tangram; expect to see different pairs in different parts of the space (i.e. to compute a d' for an 'idiosyncracy' or 'multiple equilibria' result) and also different utterances from single games closer together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nlp_utils import get_feats\n",
    "meta, raw_avg_feats, weighted_feats = get_feats(d, docs_emb, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(weighted_feats.shape[0] == meta.shape[0] )\n",
    "assert(raw_avg_feats.shape[0] == meta.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.to_csv('outputs/meta_tangrams_embeddings.csv')\n",
    "np.save('outputs/feats_tangrams_embeddings_tfidf.npy', weighted_feats)#, delimiter=',')\n",
    "np.save('outputs/feats_tangrams_embeddings_rawavg.npy', raw_avg_feats)#, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: initial distribution w/in vs. across\n",
    "TODO: 2D PCA... (traces of beginnings and ends)\n",
    "-- Connect individuals in a game with a line!\n",
    "-- Word clouds for initial and final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at tsne visualization\n",
    "TODO: there are a bunch of problems with this: a lot of the creative utterances don't exist in current embedding (e.g. \"ghostman\"), sometimes they don't converge to a noun (e.g. \"flying\"), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "tsne = TSNE(n_components = 2)\n",
    "big_pca = PCA(n_components = 50)\n",
    "viz_pca = PCA(n_components = 2)\n",
    "mds = MDS(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_viz = pd.DataFrame(\n",
    "    columns = ['gameid', 'intendedName', 'repetitionNum', 'x_tsne', 'y_tsne', 'x_mds', 'y_mds', 'feats_type']\n",
    ")\n",
    "\n",
    "for name, group in meta.groupby('intendedName') :\n",
    "    tangram_inds = np.array(group.index)\n",
    "    for feats_type in ['raw_avg', 'weighted'] :\n",
    "        feats = weighted_feats if feats_type == 'weighted' else raw_avg_feats\n",
    "        relevant_feats = feats[tangram_inds]\n",
    "        nan_rows = [i for i in range(relevant_feats.shape[0]) if pd.isna(relevant_feats[i,0])]\n",
    "        nan_insert_rows = [k - lag for (lag, k) in enumerate(nan_rows)]\n",
    "        X = np.ma.masked_invalid(relevant_feats)\n",
    "        tsne_out = tsne.fit_transform(big_pca.fit_transform(np.ma.compress_rows(X)))\n",
    "        tsne_out = np.insert(tsne_out, nan_insert_rows, np.nan, axis=0)\n",
    "        X_tsne = pd.DataFrame(tsne_out, \n",
    "                             columns = ['x_tsne', 'y_tsne'], \n",
    "                             index=tangram_inds) #X_mds, \n",
    "        X_tsne['feats_type'] = feats_type\n",
    "        embedding_viz = embedding_viz.append(pd.concat([group, X_tsne], axis = 1), \n",
    "                                             ignore_index=True, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_viz.to_csv('outputs/embeddings.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
